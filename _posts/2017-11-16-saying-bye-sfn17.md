---
layout: post
title: Algoirthms and Learning
categories: []
tags: [neuroscience]
description: 
comments: true
---

The last few days at SfN come at me, and maybe you, from all angles. Its been a deluge of information and insight into some amazing work. One 'theme' I noticed covered the mechanisms of learning from both the biological and computational perspectives.

__Synthesizing experts' theories on learning algorithms and algorithms for learning__

These concepts are interesting because how we learn shapes our memories and this interaction is fundamental to how we live and how we persist as a collective species. My own work explores memories in the brain, sometimes referred to as Engrams, whereas here we will explore learning. Though I was happy to see in Dr. Demis Hassabis's thesis a chapter on Engrams. 

![Hassabis Engrams, 80%](/assets/2017-11-16_Hassabis_thesis.png)

Presenting in the session 'From salient experience to learning and memory', Dr. Andreas L&uuml;thi explored amygdalar circuits during learning to understand algorithmically how the learned association between conditioned stimulus (CS) and unconditioned stimulus (US) is formed. 

How associative learning works mechanistically has wide implications within medicine, including associations of an experience with immense anxiety, and computer science, developing new algorithms for computers to learn with. 

A feature of this motivation stems from an old observation that principle cells in the BLA region fire very rarely. L&uuml;thi posits this to be strong inhibition from surrounding interneurons. To explore the circuitry that produces this, L&uuml;thi and his lab set out to visualize the activity of interneurons during an associative learning task. During these experiments he finds that VIP interneurons activate to the unconditioned stimulus but that subsequent US causes a decrease in VIP activity. This decrease in inhibition signal correlates with an increasing in freezing, a common behavioural output measure. The work also found that inhibiting VIP activity during associative learning causes reduced freezing later. Therefore to properly form the association VIP interneurons need to fire initially to facilitate the formation of increased activity in the principle neurons. This demonstrates a way interneurons help tune principle neurons to form the BLAs computational circuitry during associative learning. 

Moving more broadly, general computational methods for learning are being investigated at Google's DeepMind, run by Dr. Demis Hassabis. During a lecture he examined the recent interplay between neuroscience and artificial intelligence that has happened recently. 

The work at DeepMind, as Hassabis's lecture showed, recently has been to train neural networks on the game Go, and others, so well as to beat human champions. The learning of difficult games to such an extreme level has taught Go players, for example, new strategies and Hassabis argues this level of learning will broaden our collective intelligence rather than undermine human capacity.

In addition to forming neural networks to learn games, the company has started to develop networks that given 2-3 2D views of an environment can generate the entire 3D space. Right now this works for simplistic spaces but the ability to generate scene's he hopes will teach us about our own ability to imagine. And together these computer algorithms will inform us about the human experience of learning. 

These two lectures explored learning but in very different ways. The biological approach informs us on our natural algorithms and their circuit constraints for learning. The computational explores algorithms that mirror our natural ones as well as algorithms that do not. From this we can compare and contrast. Even more interesting, is the computational learning algorithms are able to advance our knowledge, as it did in strategies with Go, perhaps navigating around circuit 'blocks' in our own learning systems. 



----------

Patrick E. Steadman, MSc  
_PhD Candidate, Frankland Lab, The Hospital for Sick Children_  
_MD-PhD Student, University of Toronto_  
Neuronline: @patrick.steadman  
Twitter: [@pesteadman](http://twitter.com/pesteadman)  
Blog: [patricksteadman.ca/blog](http://patricksteadman.ca/blog) 
